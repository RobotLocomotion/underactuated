{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TKvYiJgnYExi"
   },
   "source": [
    "This notebook provides examples to go along with the [textbook](http://underactuated.csail.mit.edu/dp.html).  I recommend having both windows open, side-by-side!\n",
    "\n",
    "[Click here](http://underactuated.csail.mit.edu/drake.html#notebooks) for instructions on how to run the notebook on Deepnote and/or Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A4QOaw_zYLfI"
   },
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import HTML, clear_output, display\n",
    "from matplotlib import cm\n",
    "from pydrake.all import (DiagramBuilder, DiscreteAlgebraicRiccatiEquation,\n",
    "                         DynamicProgrammingOptions, FittedValueIteration,\n",
    "                         LinearSystem, MultilayerPerceptron,\n",
    "                         PerceptronActivationType, PeriodicBoundaryCondition,\n",
    "                         RandomGenerator, Rgba, Simulator, StartMeshcat,\n",
    "                         WrapToSystem)\n",
    "from pydrake.examples.pendulum import PendulumPlant\n",
    "\n",
    "from underactuated.double_integrator import DoubleIntegratorVisualizer\n",
    "from underactuated.jupyter import AdvanceToAndVisualize, running_as_notebook\n",
    "from underactuated.meshcat_cpp_utils import interact, plot_surface\n",
    "from underactuated.pendulum import PendulumVisualizer\n",
    "from underactuated.optimizers import Adam\n",
    "\n",
    "plt.rcParams.update({\"savefig.transparent\": True})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the visualizer (run this cell only once, each instance consumes a port)\n",
    "meshcat = StartMeshcat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Grid World\n",
    "\n",
    "The setup here is *almost* identical as the simplest version described in the notes.  The only difference is that this agent is allowed to move diagonally in a single step; this is slightly easier to code since I can have two actions (one for left/right, and another for up/down), and write the dynamics as the trivial linear system ${\\bf x}[n+1] = {\\bf u}[n].$  Only the value iteration code needs to know that the states and actions are actually restricted to the integers. I also add a very large cost when the action would be diagonal, so that it is never chosen.\n",
    "\n",
    "The obstacle (pit of despair) is provided by the method below.  Play around with it!  The rest of the code is mostly to support visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_world_example():\n",
    "    time_step = 1\n",
    "    # TODO(russt): Support discrete-time systems in the dynamic programming code, and use this properly.\n",
    "    #plant = LinearSystem(A=np.eye(2), B=np.eye(2), C=np.eye(2), D=np.zeros((2,2)), time_period=time_step)\n",
    "    # for now, just cheat because I know how to make the discrete system as a continuous that will be discretized.\n",
    "    plant = LinearSystem(A=np.zeros((2,2)), B=np.eye(2), C=np.eye(2), D=np.zeros((2,2)))\n",
    "    simulator = Simulator(plant)\n",
    "    options = DynamicProgrammingOptions()\n",
    "\n",
    "    xbins = range(0, 21)\n",
    "    ybins = range(0, 16)\n",
    "    state_grid = [set(xbins), set(ybins)]\n",
    "\n",
    "    input_grid = [set([-1, 0, 1]), set([-1, 0, 1])]\n",
    "\n",
    "    goal = [2, 8]\n",
    "\n",
    "    def obstacle(x):\n",
    "        return x[0]>=6 and x[0]<=8 and x[1]>=4 and x[1]<=7\n",
    "\n",
    "    [X, Y] = np.meshgrid(xbins, ybins)\n",
    "\n",
    "    frames=[]\n",
    "    def draw(iteration, mesh, cost_to_go, policy):\n",
    "        J = np.reshape(cost_to_go, X.shape)\n",
    "        artists = [ax.imshow(J, cmap=cm.jet)]\n",
    "        artists += [\n",
    "            ax.quiver(X,\n",
    "                      Y,\n",
    "                      np.reshape(policy[0], X.shape),\n",
    "                      np.reshape(policy[1], Y.shape),\n",
    "                      scale=1.4,\n",
    "                      scale_units='x')\n",
    "        ]\n",
    "        frames.append(artists)\n",
    "\n",
    "    if running_as_notebook:\n",
    "        options.visualization_callback = draw\n",
    "\n",
    "    def min_time_cost(context):\n",
    "        x = context.get_continuous_state_vector().CopyToVector()\n",
    "        x = np.round(x)\n",
    "        state_cost = 1\n",
    "        if obstacle(x):\n",
    "            state_cost = 10\n",
    "        if np.array_equal(x, goal):\n",
    "            state_cost = 0\n",
    "        u = plant.get_input_port(0).Eval(context)\n",
    "        action_cost = np.linalg.norm(u, 1)\n",
    "        if action_cost > 1:\n",
    "            action_cost = 10\n",
    "        return state_cost + action_cost\n",
    "\n",
    "    cost_function = min_time_cost\n",
    "    options.convergence_tol = .1;\n",
    "\n",
    "    (fig,ax) = plt.subplots(figsize=(10,6))\n",
    "    ax.set_xlabel(\"x\")\n",
    "    ax.set_ylabel(\"y\")\n",
    "    ax.set_title(\"Cost-to-Go\")\n",
    "\n",
    "    policy, cost_to_go = FittedValueIteration(simulator, cost_function,\n",
    "                                              state_grid, input_grid, time_step,\n",
    "                                              options)\n",
    "\n",
    "    draw('Final', None, cost_to_go, policy.get_output_values())\n",
    "\n",
    "    ax.invert_yaxis()\n",
    "    plt.colorbar(frames[-1][0])\n",
    "\n",
    "    print(\"generating animation...\")\n",
    "    # create animation using the animate() function\n",
    "    ani = animation.ArtistAnimation(fig, frames, interval=200, blit=True, repeat=False)\n",
    "    plt.close('all')\n",
    "\n",
    "    display(HTML(ani.to_jshtml()))\n",
    "\n",
    "grid_world_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your turn.  Change the cost.  Change the obstacles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Iteration for the Double Integrator\n",
    "\n",
    "Note that I've inserted a sleep command in the draw method to intentionally slow down the algorithm, so that you can watch the convergence in the visualizer.  If you take out the pause, it's quite fast!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DoubleIntegrator():\n",
    "    return LinearSystem(A=np.mat('0 1; 0 0'),\n",
    "                        B=np.mat('0; 1'),\n",
    "                        C=np.eye(2),\n",
    "                        D=np.zeros((2,1)))\n",
    "meshcat.Delete()\n",
    "meshcat.SetProperty('/Background', \"visible\", False)\n",
    "plant = DoubleIntegrator()\n",
    "\n",
    "def double_integrator_example(cost_function,\n",
    "                              convergence_tol,\n",
    "                              animate=True,\n",
    "                              plot=True,\n",
    "                              draw_iterations=True):\n",
    "    simulator = Simulator(plant)\n",
    "    options = DynamicProgrammingOptions()\n",
    "\n",
    "    qbins = np.linspace(-3., 3., 31)\n",
    "    qdotbins = np.linspace(-4., 4., 51)\n",
    "    state_grid = [set(qbins), set(qdotbins)]\n",
    "\n",
    "    input_limit = 1.\n",
    "    input_grid = [set(np.linspace(-input_limit, input_limit, 9))]\n",
    "    timestep = 0.01\n",
    "\n",
    "    [Q, Qdot] = np.meshgrid(qbins, qdotbins)\n",
    "\n",
    "    def draw(iteration, mesh, cost_to_go, policy):\n",
    "        # Don't draw every frame.\n",
    "        if iteration % 20 != 0:\n",
    "            return\n",
    "\n",
    "        # TODO: color by z value (e.g. cm.jet)\n",
    "        plot_surface(meshcat,\n",
    "                     'Cost-to-go',\n",
    "                     Q,\n",
    "                     Qdot,\n",
    "                     np.reshape(cost_to_go, Q.shape),\n",
    "                     wireframe=True)\n",
    "        plot_surface(meshcat,\n",
    "                     'Policy',\n",
    "                     Q,\n",
    "                     Qdot,\n",
    "                     np.reshape(policy, Q.shape),\n",
    "                     rgba=Rgba(.3, .3, .5))\n",
    "\n",
    "        # Slow down the algorithm so we can visualize the convergence.\n",
    "        sleep(0.1)\n",
    "\n",
    "    def simulate(policy):\n",
    "        # Animate the resulting policy.\n",
    "        builder = DiagramBuilder()\n",
    "        plant = builder.AddSystem(DoubleIntegrator())\n",
    "\n",
    "        vi_policy = builder.AddSystem(policy)\n",
    "        builder.Connect(plant.get_output_port(0), vi_policy.get_input_port(0))\n",
    "        builder.Connect(vi_policy.get_output_port(0), plant.get_input_port(0))\n",
    "\n",
    "        visualizer = builder.AddSystem(DoubleIntegratorVisualizer(show=False))\n",
    "        builder.Connect(plant.get_output_port(0), visualizer.get_input_port(0))\n",
    "\n",
    "        diagram = builder.Build()\n",
    "        simulator = Simulator(diagram)\n",
    "\n",
    "        simulator.get_mutable_context().SetContinuousState([-10.0, 0.0])\n",
    "\n",
    "        AdvanceToAndVisualize(simulator, visualizer, 10.)\n",
    "\n",
    "    if running_as_notebook and draw_iterations:\n",
    "        options.visualization_callback = draw\n",
    "    options.convergence_tol = convergence_tol\n",
    "\n",
    "    policy, cost_to_go = FittedValueIteration(simulator, cost_function,\n",
    "                                              state_grid, input_grid, timestep,\n",
    "                                              options)\n",
    "\n",
    "    J = np.reshape(cost_to_go, Q.shape)\n",
    "\n",
    "    plot_surface(meshcat, 'Cost-to-go', Q, Qdot, J, wireframe=True)\n",
    "\n",
    "    if animate:\n",
    "        print('Simulating...')\n",
    "        simulate(policy)\n",
    "\n",
    "    if plot:\n",
    "        fig = plt.figure(1, figsize=(9, 4))\n",
    "        ax1, ax2 = fig.subplots(1, 2)\n",
    "        ax1.set_xlabel(\"q\")\n",
    "        ax1.set_ylabel(\"qdot\")\n",
    "        ax1.set_title(\"Cost-to-Go\")\n",
    "        ax2.set_xlabel(\"q\")\n",
    "        ax2.set_ylabel(\"qdot\")\n",
    "        ax2.set_title(\"Policy\")\n",
    "        ax1.imshow(J,\n",
    "                   cmap=cm.jet,\n",
    "                   extent=(qbins[0], qbins[-1], qdotbins[-1], qdotbins[0]))\n",
    "        ax1.invert_yaxis()\n",
    "        Pi = np.reshape(policy.get_output_values(), Q.shape)\n",
    "        ax2.imshow(Pi,\n",
    "                   cmap=cm.jet,\n",
    "                   extent=(qbins[0], qbins[-1], qdotbins[-1], qdotbins[0]))\n",
    "        ax2.invert_yaxis()\n",
    "        display(plt.show())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_time_cost(context):\n",
    "    x = context.get_continuous_state_vector().CopyToVector()\n",
    "    if x.dot(x) < .05:\n",
    "        return 0.\n",
    "    return 1.\n",
    "\n",
    "\n",
    "double_integrator_example(cost_function=min_time_cost,\n",
    "                          convergence_tol=0.001,\n",
    "                          animate=True);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quadratic_regulator_cost(context):\n",
    "    x = context.get_continuous_state_vector().CopyToVector()\n",
    "    u = plant.EvalVectorInput(context, 0).CopyToVector()\n",
    "    return x.dot(x) + u.dot(u)\n",
    "\n",
    "double_integrator_example(cost_function=quadratic_regulator_cost, convergence_tol=0.1, animate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Iteration for the Simple Pendulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def pendulum_swingup_example(min_time=True, animate=True):\n",
    "    plant = PendulumPlant()\n",
    "    simulator = Simulator(plant)\n",
    "    options = DynamicProgrammingOptions()\n",
    "\n",
    "    qbins = np.linspace(0., 2. * np.pi, 51)\n",
    "    qdotbins = np.linspace(-10., 10., 51)\n",
    "    state_grid = [set(qbins), set(qdotbins)]\n",
    "    options.periodic_boundary_conditions = [\n",
    "        PeriodicBoundaryCondition(0, 0., 2. * np.pi),\n",
    "    ]\n",
    "    input_limit = 3.\n",
    "    input_grid = [set(np.linspace(-input_limit, input_limit, 9))]\n",
    "    timestep = 0.01\n",
    "\n",
    "    [Q, Qdot] = np.meshgrid(qbins, qdotbins)\n",
    "\n",
    "    meshcat.Delete()\n",
    "    meshcat.SetProperty(\"/Background\", \"visible\", False)\n",
    "\n",
    "    def draw(iteration, mesh, cost_to_go, policy):\n",
    "        # Don't draw every frame.\n",
    "        if iteration % 20 != 0:\n",
    "            return\n",
    "\n",
    "        plot_surface(meshcat,\n",
    "                     'Cost-to-go',\n",
    "                     Q,\n",
    "                     Qdot,\n",
    "                     np.reshape(cost_to_go, Q.shape),\n",
    "                     wireframe=True)\n",
    "        plot_surface(meshcat,\n",
    "                     'Policy',\n",
    "                     Q,\n",
    "                     Qdot,\n",
    "                     np.reshape(policy, Q.shape),\n",
    "                     rgba=Rgba(.3, .3, .5))\n",
    "\n",
    "        # Slow down the algorithm so we can visualize the convergence.\n",
    "        sleep(0.1)\n",
    "\n",
    "    def simulate(policy):\n",
    "        # Animate the resulting policy.\n",
    "        builder = DiagramBuilder()\n",
    "        pendulum = builder.AddSystem(PendulumPlant())\n",
    "\n",
    "        wrap = builder.AddSystem(WrapToSystem(2))\n",
    "        wrap.set_interval(0, 0, 2*np.pi)\n",
    "        builder.Connect(pendulum.get_output_port(0), wrap.get_input_port(0))\n",
    "        vi_policy = builder.AddSystem(policy)\n",
    "        builder.Connect(wrap.get_output_port(0), vi_policy.get_input_port(0))\n",
    "        builder.Connect(vi_policy.get_output_port(0),\n",
    "                        pendulum.get_input_port(0))\n",
    "\n",
    "        visualizer = builder.AddSystem(\n",
    "            PendulumVisualizer(show=False))\n",
    "        builder.Connect(pendulum.get_output_port(0),\n",
    "                        visualizer.get_input_port(0))\n",
    "\n",
    "        diagram = builder.Build()\n",
    "        simulator = Simulator(diagram)\n",
    "        simulator.get_mutable_context().SetContinuousState([0.1, 0.0])\n",
    "\n",
    "        AdvanceToAndVisualize(simulator, visualizer, 8.)\n",
    "\n",
    "    if running_as_notebook:\n",
    "        options.visualization_callback = draw\n",
    "\n",
    "    def min_time_cost(context):\n",
    "        x = context.get_continuous_state_vector().CopyToVector()\n",
    "        x[0] = x[0] - np.pi\n",
    "        if x.dot(x) < .05:\n",
    "            return 0.\n",
    "        return 1.\n",
    "\n",
    "    def quadratic_regulator_cost(context):\n",
    "        x = context.get_continuous_state_vector().CopyToVector()\n",
    "        x[0] = x[0] - np.pi\n",
    "        u = plant.EvalVectorInput(context, 0).CopyToVector()\n",
    "        return 2 * x.dot(x) + u.dot(u)\n",
    "\n",
    "    if min_time:\n",
    "        cost_function = min_time_cost\n",
    "        options.convergence_tol = 0.001\n",
    "    else:\n",
    "        cost_function = quadratic_regulator_cost\n",
    "        options.convergence_tol = 0.1\n",
    "\n",
    "    policy, cost_to_go = FittedValueIteration(simulator, cost_function,\n",
    "                                              state_grid, input_grid, timestep,\n",
    "                                              options)\n",
    "\n",
    "    J = np.reshape(cost_to_go, Q.shape)\n",
    "\n",
    "    plot_surface(meshcat, 'Cost-to-go', Q, Qdot, J, wireframe=True)\n",
    "\n",
    "    if animate:\n",
    "        print('Simulating...')\n",
    "        simulate(policy)\n",
    "\n",
    "    fig = plt.figure(figsize=(9, 4))\n",
    "    ax1, ax2 = fig.subplots(1, 2)\n",
    "    ax1.set_xlabel(\"q\")\n",
    "    ax1.set_ylabel(\"qdot\")\n",
    "    ax1.set_title(\"Cost-to-Go\")\n",
    "    ax2.set_xlabel(\"q\")\n",
    "    ax2.set_ylabel(\"qdot\")\n",
    "    ax2.set_title(\"Policy\")\n",
    "    ax1.imshow(J,\n",
    "               cmap=cm.jet, aspect='auto',\n",
    "               extent=(qbins[0], qbins[-1], qdotbins[-1], qdotbins[0]))\n",
    "    ax1.invert_yaxis()\n",
    "    Pi = np.reshape(policy.get_output_values(), Q.shape)\n",
    "    ax2.imshow(Pi,\n",
    "               cmap=cm.jet, aspect='auto',\n",
    "               extent=(qbins[0], qbins[-1], qdotbins[-1], qdotbins[0]))\n",
    "    ax2.invert_yaxis()\n",
    "    display(plt.show())\n",
    "\n",
    "\n",
    "pendulum_swingup_example(min_time=True, animate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Fitted Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the double integrator\n",
    "A = np.array([[0., 1.], [0., 0.]])\n",
    "B = np.array([[0.], [1.]])\n",
    "Q = 0.1*np.eye(2)\n",
    "R = np.eye(1)\n",
    "\n",
    "# vectorized\n",
    "def min_time_cost(x, u):\n",
    "    return 1.0 - np.isclose(x, np.zeros((2,1))).all(axis=0)\n",
    "\n",
    "def quadratic_regulator_cost(x, u):\n",
    "    return (x * (Q @ x)).sum(axis=0) + (u * (R @ u)).sum(axis=0)\n",
    "\n",
    "def min_time_solution(x):\n",
    "    # Caveat: this does not take the time discretization (zero-order hold on u) into account.\n",
    "    q = x[0,:]\n",
    "    qdot = x[1,:]\n",
    "    # mask indicates that we are in the regime where u = +1.\n",
    "    mask = ((qdot < 0) & (2 * q <=\n",
    "                          (qdot**2))) | ((qdot >= 0) & (2 * q < -(qdot**2)))\n",
    "    T = np.empty(q.size)\n",
    "    T[mask] = 2*np.sqrt(.5*qdot[mask]**2 - q[mask]) - qdot[mask]\n",
    "    T[~mask] = qdot[~mask] + 2*np.sqrt(.5*qdot[~mask]**2 + q[~mask])\n",
    "    return T\n",
    "\n",
    "def quadratic_regulator_solution(x, timestep, gamma=1):\n",
    "    S = DiscreteAlgebraicRiccatiEquation(A=np.sqrt(gamma) *\n",
    "                                         (np.eye(2) + timestep * A),\n",
    "                                         B=timestep * B,\n",
    "                                         Q=timestep * Q,\n",
    "                                         R=timestep * R / gamma)\n",
    "    return (x * (S @ x)).sum(axis=0)\n",
    "\n",
    "def plot_and_compare(mlp, context, running_cost, timestep, gamma=1.0):\n",
    "    x1s = np.linspace(-5,5,31)\n",
    "    x2s = np.linspace(-4,4,51)\n",
    "    X1s, X2s = np.meshgrid(x1s, x2s)\n",
    "    N = X1s.size\n",
    "    X = np.vstack((X1s.flatten(), X2s.flatten()))\n",
    "    J = np.zeros((1,N))\n",
    "\n",
    "    mlp.BatchOutput(context, X, J)\n",
    "\n",
    "    plot_surface(meshcat,\n",
    "                 \"Jhat\",\n",
    "                 X1s,\n",
    "                 X2s,\n",
    "                 J.reshape(X1s.shape),\n",
    "                rgba=Rgba(0,0,1),\n",
    "                 wireframe=True)\n",
    "\n",
    "    if running_cost == min_time_cost:\n",
    "        Jd = min_time_solution(X)\n",
    "    elif running_cost == quadratic_regulator_cost:\n",
    "        Jd = quadratic_regulator_solution(X, timestep, gamma)\n",
    "\n",
    "    plot_surface(meshcat,\n",
    "                 \"J_desired\",\n",
    "                 X1s,\n",
    "                 X2s,\n",
    "                 Jd.reshape(X1s.shape),\n",
    "                 rgba=Rgba(1, 0, 0),\n",
    "                 wireframe=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's simply evaluate how well the network can fit the known cost-to-go functions (using supervised learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SupervisedDemo(running_cost, timestep):\n",
    "    x1s = np.linspace(-5,5,51)\n",
    "    x2s = np.linspace(-4,4,51)\n",
    "    X1s, X2s = np.meshgrid(x1s, x2s)\n",
    "    N = X1s.size\n",
    "    X = np.vstack((X1s.flatten(), X2s.flatten()))\n",
    "\n",
    "    if running_cost == min_time_cost:\n",
    "        Jd = min_time_solution(X)\n",
    "    elif running_cost == quadratic_regulator_cost:\n",
    "        Jd = quadratic_regulator_solution(X, timestep)\n",
    "\n",
    "    Jd = Jd.reshape((1,N))\n",
    "\n",
    "    mlp = MultilayerPerceptron(\n",
    "        [2,100,100,1],\n",
    "        [PerceptronActivationType.kReLU, \n",
    "         PerceptronActivationType.kReLU,\n",
    "         PerceptronActivationType.kIdentity])\n",
    "    context = mlp.CreateDefaultContext()\n",
    "    generator = RandomGenerator(152)\n",
    "    mlp.SetRandomContext(context, generator)\n",
    "\n",
    "    optimizer = Adam(mlp.GetMutableParameters(context))\n",
    "\n",
    "    dloss_dparams = np.zeros(mlp.num_parameters())\n",
    "    last_loss = np.inf\n",
    "    for epoch in range(1000 if running_as_notebook else 2):\n",
    "        loss = mlp.BackpropagationMeanSquaredError(context, X, Jd,\n",
    "                                                    dloss_dparams)\n",
    "        clear_output(wait=True)\n",
    "        print(f\"loss = {loss}\")\n",
    "        if np.linalg.norm(last_loss - loss) < 0.0001:\n",
    "            break\n",
    "        last_loss = loss\n",
    "        optimizer.step(loss, dloss_dparams)\n",
    "\n",
    "    plot_and_compare(mlp, context, running_cost, timestep)\n",
    "\n",
    "meshcat.Delete()\n",
    "SupervisedDemo(min_time_cost, 0.1)\n",
    "#SupervisedDemo(quadratic_regulator_cost, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete time, continuous state, discrete action\n",
    "\n",
    "This is the standard \"fitted value iteration\" algorithm with a multilayer perceptron (MLP) as the function approximator, and a single step of gradient descent performed on each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FittedValueIteration(running_cost, timestep):\n",
    "    x1s = np.linspace(-5,5,31)\n",
    "    x2s = np.linspace(-4,4,31)\n",
    "    us = np.linspace(-1,1,9)\n",
    "    Us, X1s, X2s = np.meshgrid(us, x1s, x2s, indexing='ij')\n",
    "    XwithU = np.vstack((X1s.flatten(), X2s.flatten()))\n",
    "    UwithX = Us.flatten().reshape(1,-1)\n",
    "    Nx = x1s.size * x2s.size\n",
    "    X = XwithU[:,:Nx]\n",
    "    N = X1s.size\n",
    "\n",
    "    Xnext = XwithU + timestep * (A @ XwithU + B @ UwithX)\n",
    "    G = timestep*running_cost(XwithU, UwithX)\n",
    "    Jnext = np.zeros((1,N))\n",
    "    Jd = np.zeros((1,Nx))\n",
    "\n",
    "    mlp = MultilayerPerceptron(\n",
    "        [2,100,100,1],\n",
    "        [PerceptronActivationType.kReLU,\n",
    "         PerceptronActivationType.kReLU,\n",
    "         PerceptronActivationType.kIdentity])\n",
    "    context = mlp.CreateDefaultContext()\n",
    "    generator = RandomGenerator(123)\n",
    "    mlp.SetRandomContext(context, generator)\n",
    "\n",
    "    optimizer = Adam(mlp.GetMutableParameters(context))\n",
    "\n",
    "    gamma = 0.9\n",
    "    plot_and_compare(mlp, context, running_cost, timestep, gamma)\n",
    "    dloss_dparams = np.zeros(mlp.num_parameters())\n",
    "    last_loss = np.inf\n",
    "    for epoch in range(500 if running_as_notebook else 2):\n",
    "        mlp.BatchOutput(context, Xnext, Jnext)\n",
    "        Jd[:] = np.min((G + gamma*Jnext).reshape(us.size, Nx), axis=0)\n",
    "        for i in range(100 if running_as_notebook else 2):\n",
    "            loss = mlp.BackpropagationMeanSquaredError(\n",
    "                context, X, Jd, dloss_dparams)\n",
    "            optimizer.step(loss, dloss_dparams)\n",
    "        if np.linalg.norm(last_loss - loss) < 1e-8:\n",
    "            break\n",
    "        last_loss = loss\n",
    "        clear_output(wait=True)\n",
    "        print(f\"epoch {epoch}: loss = {loss}\")\n",
    "        if epoch%10 == 0:\n",
    "            plot_and_compare(mlp, context, running_cost, timestep, gamma)\n",
    "\n",
    "    plot_and_compare(mlp, context, running_cost, timestep, gamma)\n",
    "\n",
    "FittedValueIteration(min_time_cost, 0.1)\n",
    "#FittedValueIteration(quadratic_regulator_cost, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even simpler, let's do linear function approximation with the quadratic form for the LQR problem..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Underactuated Robotics - The Simple Pendulum.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
