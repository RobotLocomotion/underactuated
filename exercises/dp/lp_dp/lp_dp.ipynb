{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "8d0a492a-7932-4e43-99e7-9568f6feb88a",
    "deepnote_cell_type": "markdown",
    "id": "TKvYiJgnYExi"
   },
   "source": [
    "This notebook provides examples to go along with the [textbook](http://underactuated.csail.mit.edu/dp.html).  I recommend having both windows open, side-by-side!\n",
    "\n",
    "[Click here](http://underactuated.csail.mit.edu/drake.html#notebooks) for instructions on how to run the notebook on Deepnote and/or Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00001-142a6d01-32b5-4a15-aa12-c164d4e5d662",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 6,
    "execution_start": 1644279886355,
    "id": "A4QOaw_zYLfI",
    "source_hash": "6663f6e1"
   },
   "outputs": [],
   "source": [
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import HTML, display\n",
    "from matplotlib import cm\n",
    "from pydrake.all import (DynamicProgrammingOptions, FittedValueIteration,\n",
    "                         LinearSystem, Simulator)\n",
    "\n",
    "from underactuated.jupyter import running_as_notebook\n",
    "\n",
    "\n",
    "plt.rcParams.update({\"savefig.transparent\": True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00003-e23464ff-88d3-4b47-958a-88c8f84dfc52",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "# The Grid World\n",
    "\n",
    "We have seen in class that we can obtain value function using FittedValueIteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_world_example():\n",
    "    time_step = 1\n",
    "    # TODO(russt): Support discrete-time systems in the dynamic programming code, and use this properly.\n",
    "    #plant = LinearSystem(A=np.eye(2), B=np.eye(2), C=np.eye(2), D=np.zeros((2,2)), time_period=time_step)\n",
    "    # for now, just cheat because I know how to make the discrete system as a continuous that will be discretized.\n",
    "    plant = LinearSystem(A=np.zeros((2,2)), B=np.eye(2), C=np.eye(2), D=np.zeros((2,2)))\n",
    "    simulator = Simulator(plant)\n",
    "    options = DynamicProgrammingOptions()\n",
    "\n",
    "    xbins = range(0, 21)\n",
    "    ybins = range(0, 16)\n",
    "    state_grid = [set(xbins), set(ybins)]\n",
    "\n",
    "    input_grid = [set([-1, 0, 1]), set([-1, 0, 1])]\n",
    "\n",
    "    goal = [2, 8]\n",
    "\n",
    "    def obstacle(x):\n",
    "        return x[0]>=6 and x[0]<=8 and x[1]>=4 and x[1]<=7\n",
    "\n",
    "    [X, Y] = np.meshgrid(xbins, ybins)\n",
    "\n",
    "    frames=[]\n",
    "    def draw(iteration, mesh, cost_to_go, policy):\n",
    "        J = np.reshape(cost_to_go, X.shape)\n",
    "        artists = [ax.imshow(J, cmap=cm.jet)]\n",
    "        artists += [\n",
    "            ax.quiver(X,\n",
    "                      Y,\n",
    "                      np.reshape(policy[0], X.shape),\n",
    "                      np.reshape(policy[1], Y.shape),\n",
    "                      scale=1.4,\n",
    "                      scale_units='x')\n",
    "        ]\n",
    "        frames.append(artists)\n",
    "\n",
    "    if running_as_notebook:\n",
    "        options.visualization_callback = draw\n",
    "\n",
    "    def min_time_cost(context):\n",
    "        x = context.get_continuous_state_vector().CopyToVector()\n",
    "        x = np.round(x)\n",
    "        state_cost = 1\n",
    "        if obstacle(x):\n",
    "            state_cost = 10\n",
    "        if np.array_equal(x, goal):\n",
    "            state_cost = 0\n",
    "        u = plant.get_input_port(0).Eval(context)\n",
    "        action_cost = np.linalg.norm(u, 1)\n",
    "        if action_cost > 1:\n",
    "            action_cost = 10\n",
    "        return state_cost + action_cost\n",
    "\n",
    "    cost_function = min_time_cost\n",
    "    options.convergence_tol = .1;\n",
    "\n",
    "    (fig,ax) = plt.subplots(figsize=(10,6))\n",
    "    ax.set_xlabel(\"x\")\n",
    "    ax.set_ylabel(\"y\")\n",
    "    ax.set_title(\"Cost-to-Go\")\n",
    "\n",
    "    policy, cost_to_go = FittedValueIteration(simulator, cost_function,\n",
    "                                              state_grid, input_grid, time_step,\n",
    "                                              options)\n",
    "\n",
    "    draw('Final', None, cost_to_go, policy.get_output_values())\n",
    "\n",
    "    ax.invert_yaxis()\n",
    "    plt.colorbar(frames[-1][0])\n",
    "\n",
    "    print(\"generating animation...\")\n",
    "    # create animation using the animate() function\n",
    "    ani = animation.ArtistAnimation(fig, frames, interval=200, blit=True, repeat=False)\n",
    "    plt.close('all')\n",
    "\n",
    "    display(HTML(ani.to_jshtml()))\n",
    "\n",
    "grid_world_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00005-ed008a8e-b6be-4b57-9498-f0010fb963ab",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Linear Programming for Dynamic Programming\n",
    "\n",
    "For our discrete grid world, let's try to obtain the optimal cost-to-go using [linear programming](http://underactuated.csail.mit.edu/dp.html#LP). Linear Programming is an optimization program with linear objective functions as well as linear equality and inequality constraints. If you are not familiar with optimization, you could take a look at the linear programming [tutorial](https://github.com/RobotLocomotion/drake/blob/master/tutorials/linear_program.ipynb) in Drake. The following cells are setting up the grid world and the transition matrix $T$ in eq(14) in the textbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbins = range(0, 21)\n",
    "ybins = range(0, 21)\n",
    "[X, Y] = np.meshgrid(xbins, ybins)\n",
    "states = np.vstack((X.reshape(441), Y.reshape(441)))\n",
    "\n",
    "[ux, uy] = np.meshgrid([-1, 0, 1], [-1, 0, 1])\n",
    "inputs = np.vstack((ux.reshape(9), uy.reshape(9)))\n",
    "\n",
    "goal = [2, 8]\n",
    "\n",
    "def obstacle(x):\n",
    "    return x[0]>=6 and x[0]<=8 and x[1]>=4 and x[1]<=7\n",
    "\n",
    "A = np.eye(2)\n",
    "B = np.eye(2)\n",
    "\n",
    "input_dim = inputs.shape[1]\n",
    "state_dim = states.shape[1]\n",
    "\n",
    "T = np.zeros([state_dim, state_dim, input_dim])\n",
    "\n",
    "for i in range(input_dim):\n",
    "    for j in range(state_dim):\n",
    "        next_state = A @ states[:, j] + B @ inputs[:, i]\n",
    "        ind = np.argmin(np.linalg.norm(states.T - next_state, axis=1))\n",
    "        T[j, ind, i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_time_cost(x, u):\n",
    "    state_cost = 1\n",
    "    if obstacle(x):\n",
    "        state_cost = 10\n",
    "    if np.array_equal(x, goal):\n",
    "        state_cost = 0\n",
    "    action_cost = np.linalg.norm(u, 1)\n",
    "    if action_cost > 1:\n",
    "        action_cost = 10\n",
    "    return state_cost + action_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's your turn to code up the linear program for solving the optimal cost-to-go. These Drake [tutorials](https://github.com/RobotLocomotion/drake/tree/master/tutorials) could be super helpful for setting up the optimization program. To deal with numerical instability, you should use a discount factor $\\gamma$ for the Bellman update: $$ J \\leq l(a) + \\gamma T(a) J, \\quad \\forall a.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydrake.solvers.mathematicalprogram import MathematicalProgram, Solve\n",
    "import numpy as np\n",
    "\n",
    "# Create an empty MathematicalProgram named prog (with no decision variables,\n",
    "# constraints or costs)\n",
    "prog = MathematicalProgram()\n",
    "J = prog.NewContinuousVariables(state_dim, \"J\")\n",
    "\n",
    "gamma = 0.99999\n",
    "\n",
    "for i in range(input_dim):\n",
    "    l = np.zeros(state_dim)\n",
    "    for j in range(state_dim):\n",
    "        ## Calculate\n",
    "        l[j] = 0 # modify here\n",
    "        ## Modify here\n",
    "        ## Add Constraint for each entry of J\n",
    "        \n",
    "\n",
    "c = np.ones(state_dim)\n",
    "\n",
    "## Modify here\n",
    "## Add cost to prog\n",
    "\n",
    "\n",
    "result = Solve(prog)\n",
    "J_value = np.reshape(result.GetSolution(J), X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the value function you calculated using LP. It should be similiar to the plot obtained from FittedValueIteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(fig, ax) = plt.subplots()\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.set_title(\"Cost-to-Go\")\n",
    "k = ax.imshow(J_value, cmap=cm.jet)\n",
    "ax.invert_yaxis()\n",
    "plt.colorbar(k)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograding\n",
    "You can check your work by running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from underactuated.exercises.dp.lp_dp.test_lp_dp import Testlpdp\n",
    "from underactuated.exercises.grader import Grader\n",
    "Grader.grade_output([Testlpdp], [locals()], 'results.json')\n",
    "Grader.print_test_results('results.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "colab": {
   "collapsed_sections": [],
   "name": "Underactuated Robotics - The Simple Pendulum.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "deepnote": {},
  "deepnote_execution_queue": [
   {
    "cellId": "7e48f51e-27be-498c-a2f2-acd4fd8f9b3d",
    "msgId": "b56161c4-32df-41a5-b367-21e9851dd122",
    "sessionId": "8d4816bc-be65-4075-8f8d-24206f99532d"
   }
  ],
  "deepnote_notebook_id": "46806028-42c2-4d13-b71d-1e032cd929de",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
