{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t6bQDnCwVec8"
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import sys\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "# Install drake (and underactuated).\n",
    "if 'google.colab' in sys.modules and importlib.util.find_spec('underactuated') is None:\n",
    "    urlretrieve(f\"http://underactuated.csail.mit.edu/scripts/setup/setup_underactuated_colab.py\",\n",
    "                \"setup_underactuated_colab.py\")\n",
    "    from setup_underactuated_colab import setup_underactuated\n",
    "    setup_underactuated(underactuated_sha='1dca1b915977aeec9f327d61aaeac4cfb9c6b408', drake_version='0.25.0', drake_build='releases')\n",
    "\n",
    "from IPython import get_ipython\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import meshcat\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "\n",
    "from pydrake.all import DiagramBuilder, LinearSystem, Simulator, WrapToSystem, DiscreteAlgebraicRiccatiEquation\n",
    "from pydrake.systems.controllers import (DynamicProgrammingOptions,\n",
    "                                         FittedValueIteration, PeriodicBoundaryCondition)\n",
    "from pydrake.systems.pyplot_visualizer import PyPlotVisualizer\n",
    "from pydrake.systems.framework import Context, PortDataType\n",
    "from pydrake.math import BarycentricMesh\n",
    "from pydrake.systems.primitives import BarycentricMeshSystem\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import underactuated\n",
    "from underactuated.jupyter import AdvanceToAndVisualize, SetupMatplotlibBackend, running_as_notebook\n",
    "from underactuated.meshcat_utils import plot_surface\n",
    "\n",
    "plt_is_interactive = SetupMatplotlibBackend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYTV8z5cAI4J"
   },
   "source": [
    "## Problem Description\n",
    "In this problem we will be implementing the fitted value iteration algorithm and using it to solve the quadratic cost double integrator. While Drake provides an easy to use implementation of fitted value iteration, here we will be using a neural net to approximate the cost-to-go function instead. We will then use our cost-to-go function approximation to generate a control policy. \n",
    "\n",
    "**These are the main steps of the notebook:**\n",
    "1. Implement the neural network in order to approximate the cost-to-go function.\n",
    "2. Write the target network update in the training loop.\n",
    "3. Extract the policy from the cost-to-go function estimate.\n",
    "4. Answer the written questions at the bottom and submit them to Gradescope as a .pdf file.\n",
    "\n",
    "Note that in order to pass the autograder, you do not need to tune any hyperparameters! This includes random seeds, learning rate, training iterations, and initializations. Feel free to play around with these, but set them to the original values when you want to test with the autograder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-_ELO9RBYAa"
   },
   "source": [
    "## Plotting\n",
    "\n",
    "The cells below implement plotting for the double integrator. You do not need to understand these to do the problem, but they will be used to visualize the system so feel free to take a look - it might come in handy when you implement your final project. These are slightly modified versions of classes implemented in pydrake. You can find some other visualizers for things like the pendulum or the 2d quadrotor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DraZ3Vwa5py4"
   },
   "outputs": [],
   "source": [
    "def set_seed(seed=0):\n",
    "  np.random.seed(seed)\n",
    "  torch.random.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fKDfSOoQaSLb"
   },
   "outputs": [],
   "source": [
    "class Brick(object):\n",
    "    WIDTH = 2.\n",
    "    HEIGHT = 1.\n",
    "    def __init__(self):\n",
    "        self.patch = plt.Rectangle((0.0, 0.0),\n",
    "                                   self.WIDTH,\n",
    "                                   self.HEIGHT,\n",
    "                                   fc=\"#A31F34\",\n",
    "                                   ec=\"k\")\n",
    "        self.set_state(0.)\n",
    "\n",
    "    def set_state(self, x):\n",
    "        self.patch.set_x(x - self.WIDTH / 2)  # Center at x\n",
    "\n",
    "    @classmethod\n",
    "    def add_to_axes(cls, ax):\n",
    "        brick = cls()\n",
    "        ax.add_patch(brick.patch)\n",
    "        return brick\n",
    "        \n",
    "class DoubleIntegratorVisualizer(PyPlotVisualizer):\n",
    "\n",
    "    # Limits of view port\n",
    "    XLIM = (-10., 10.)\n",
    "    YLIM = (-6., 6.)\n",
    "    TICK_DIMS = (0.2, 0.8)\n",
    "\n",
    "    def __init__(self, ax=None, show=None):\n",
    "        PyPlotVisualizer.__init__(self, ax=ax, show=show)\n",
    "        self.DeclareInputPort(PortDataType.kVectorValued, 2)\n",
    "\n",
    "        self.ax.set_xlim(*self.XLIM)\n",
    "        self.ax.set_ylim(*self.YLIM)\n",
    "        self.ax.set_aspect(\"auto\")\n",
    "\n",
    "        self._make_background()\n",
    "        self.brick = Brick.add_to_axes(self.ax)\n",
    "\n",
    "    def _make_background(self):\n",
    "        # x-axis\n",
    "        self.ax.plot(self.XLIM, np.zeros_like(self.XLIM), \"k\")\n",
    "        # tick mark centered at the origin\n",
    "        tick_pos = -0.5 * np.asarray(self.TICK_DIMS)\n",
    "        self.ax.add_patch(plt.Rectangle(tick_pos, *self.TICK_DIMS, fc=\"k\"))\n",
    "\n",
    "    def draw(self, context):\n",
    "        try:\n",
    "            x = self.EvalVectorInput(context, 0).get_value()[0]\n",
    "            self.ax.set_title(\"t = {:.1f}\".format(context.get_time()))\n",
    "        except TypeError:\n",
    "            x = context[0]\n",
    "        self.brick.set_state(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "auHY36I7B01H"
   },
   "source": [
    "## Defining our Cost Function Approximator\n",
    "\n",
    "In order to perform value iteration in a continuous state space, we will use a neural network to approximate the cost-to-go $J(x)$. Our network will take as input the double integrator state $x = [q, \\dot q]$, and output $J(x)$. Implement the following architecture:\n",
    "\n",
    "- Linear layer with 2 inputs, 120 outputs\n",
    "- Leaky relu nonlinearity\n",
    "- Linear layer with 120 inputs, 84 outputs\n",
    "- Leaky relu nonlinearity\n",
    "- Linear layer with 84 inputs, 1 ouput\n",
    "\n",
    "An example placeholder layer is implemented for you, and we show you how to call it functionally in `forward` with a leaky relu nonlinearity. This is an alternative way to define a network instead of stacking layers using `nn.Sequential`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cxgwSvEC5-qa"
   },
   "outputs": [],
   "source": [
    "# Define the function approximator for J\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        ### TODO ###\n",
    "        self.fc1 = nn.Linear(2, 1) # placeholder\n",
    "\n",
    "    def forward(self, x):\n",
    "        ### TODO ###\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJqf-kNwKqSd"
   },
   "source": [
    "## Defining the state-space and cost function\n",
    "\n",
    "Below we define two costs. We have the quadratic regulator cost:\n",
    "\n",
    "$$g(x, u) = x^T Q x + u^T R u$$\n",
    "\n",
    "And we have the minimum time cost:\n",
    "\n",
    "$$g(x) = 1 \\text{  if  } x = 0, \\text{  else  } 0$$\n",
    "\n",
    "These will both seek to stabilize the system to the origin $x = 0$. The solution to the quadratic regulator cost makes use of drake to generate to solve the algebraic riccati equation and generate the solution matrix $S$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uja0KdGxC_4F"
   },
   "outputs": [],
   "source": [
    "Q = torch.eye(2)\n",
    "R = torch.eye(1)\n",
    "def quadratic_regulator_cost(xt, ut):\n",
    "    return xt.matmul(Q.matmul(xt.transpose(-2,-1))) + ut.matmul(R.matmul(ut.transpose(-2,-1)))\n",
    "\n",
    "def quadratic_regulator_solution(xt, timestep):\n",
    "  S = DiscreteAlgebraicRiccatiEquation(A=(np.eye(2)+timestep*A.numpy()),\n",
    "                                       B=timestep*B.numpy(),\n",
    "                                       Q=Q, R=R)\n",
    "  return xt.matmul(torch.from_numpy(S).float().matmul(xt.transpose(-2,-1)))\n",
    "\n",
    "# Define the cost function\n",
    "def min_time_cost(xt):\n",
    "    at_goal = torch.isclose(xt, torch.zeros(1,2), atol=1e-3)\n",
    "    # cost = 1 if ~at_goal * [1;1] >= 1, 0 otherwise.\n",
    "    return torch.min((~at_goal).float().matmul(torch.ones(2,1)), torch.ones(1))\n",
    "\n",
    "def min_time_solution(xt):\n",
    "  # Caveat: this does not take the time discretization (zero-order hold on u) into account.\n",
    "  q = xt[:,:,0]\n",
    "  qdot = xt[:,:,1]\n",
    "  # mask indicates that we are in the regime where u = +1.\n",
    "  mask = ((qdot < 0) & (2*q <= qdot.pow(2))) | ((qdot >= 0) & (2*q < -qdot.pow(2)))\n",
    "  T = torch.empty(q.size())\n",
    "  T[mask] = 2*(.5*(qdot[mask].pow(2)) - q[mask]).sqrt() - qdot[mask]\n",
    "  T[~mask] = qdot[~mask] + 2*(.5*(qdot[~mask].pow(2)) + q[~mask]).sqrt()\n",
    "  return T.unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qo2JYrug9UHS"
   },
   "source": [
    "In the cell below, we set the state space samples we will use to train the network, the action space discretization, and the timestep increment. We produce a mesh over state space, and for each state in this mesh we produce the cost $g(x, u)$ and the next state for each action taken. Because the double integrator is a linear system, generating the next states can be done very simply using the coefficient matrices $A$ and $B$! There is also a method below that we will use to plot the estimated cost surface produced by our neural net, and plot the analytical cost surface. Feel free to take a look if you're interested. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "klEO8sBoBjuq"
   },
   "outputs": [],
   "source": [
    "# Define state-space and timestep increment, and cost type\n",
    "num_x1s = 31\n",
    "num_x2s = 51\n",
    "num_states = num_x1s*num_x2s\n",
    "num_actions = 9\n",
    "x1s = torch.linspace(-3,3,num_x1s)\n",
    "x2s = torch.linspace(-3,3,num_x2s)\n",
    "us = torch.linspace(-1,1,num_actions)\n",
    "timestep = 0.1\n",
    "\n",
    "# Define batch of states with actions\n",
    "X1s, X2s = torch.meshgrid(x1s, x2s)\n",
    "X = torch.stack((X1s.flatten(), X2s.flatten()), 1).unsqueeze(1)\n",
    "X1s_wU, X2s_wU, Us_wX = torch.meshgrid(x1s, x2s, us)\n",
    "XwithU = torch.stack((X1s_wU.flatten(0,1), X2s_wU.flatten(0,1)), 2).unsqueeze(2)\n",
    "UwithX = Us_wX.flatten(0,1).unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "# Define coefficient matrices for double integrator state-space representation\n",
    "A = torch.tensor([[0., 1.], [0., 0.]])\n",
    "B = torch.tensor([[0.], [1.]])\n",
    "At = A.transpose(0, 1)\n",
    "Bt = B.transpose(0, 1)\n",
    "\n",
    "# Generate next states and costs\n",
    "Xnext = XwithU + timestep * (XwithU.matmul(At) + UwithX.matmul(Bt))\n",
    "G_quadratic = timestep*quadratic_regulator_cost(XwithU, UwithX)\n",
    "G_min_time = timestep*min_time_cost(XwithU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ozYW88FGD8FQ"
   },
   "outputs": [],
   "source": [
    "def plot_and_compare(net, timestep, min_time=False):\n",
    "\n",
    "  with torch.no_grad():\n",
    "    J = net.forward(X)\n",
    "\n",
    "  # Plot estimated cost surface\n",
    "  fig = plt.figure(figsize=(9, 4))\n",
    "  ax1, ax2 = fig.subplots(1, 2, subplot_kw=dict(projection='3d'))\n",
    "  ax1.set_xlabel(\"q\")\n",
    "  ax1.set_ylabel(\"qdot\")\n",
    "  ax1.set_title(\"Estimated Cost-to-Go\")\n",
    "  ax1.plot_surface(X1s, X2s, J.view(X1s.size()).detach().numpy(), rstride=1, cstride=1, cmap=cm.jet)\n",
    "  \n",
    "  # Plot analytical cost surface\n",
    "  if min_time:\n",
    "    Jd = min_time_solution(X)\n",
    "  else:\n",
    "    Jd = quadratic_regulator_solution(X, timestep)\n",
    "  ax2.set_xlabel(\"q\")\n",
    "  ax2.set_ylabel(\"qdot\")\n",
    "  ax2.set_title(\"Analytical Cost-to-Go\")\n",
    "  ax2.plot_surface(X1s, X2s, Jd.view(X1s.size()).detach().numpy(), rstride=1, cstride=1, cmap=cm.jet)\n",
    "    \n",
    "  # Score is worst absolute different (e.g. infinity-norm) of the samples\n",
    "  criterion = nn.MSELoss()\n",
    "  score = criterion(J, Jd).item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TWxDj23hMT5m"
   },
   "source": [
    "## Fitting the Cost-to-go Function\n",
    "\n",
    "We can now fit the cost-to-go function. The idea is to train a single neural network to approximate the cost-to-go function, and use a target network to represent our \"latest cost-to-go function estimate\". In the `solve` method below, write one line of code that takes the weights from `net` and loads them into `target_net` before computing the targets. You can access the state of a networks weights using `net.state_dict()`. This is useful for saving files with your model weights (see [saving in pytorch](https://pytorch.org/tutorials/beginner/saving_loading_models.html)). Weights can be loaded into a model using `net.load_state_dict()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SC3MmR4YVvuY"
   },
   "outputs": [],
   "source": [
    "def solve(net, target_net, loss_function, optimizer, min_time=False):\n",
    "\n",
    "  if min_time:\n",
    "    G = G_min_time\n",
    "  else:\n",
    "    G = G_quadratic\n",
    "  \n",
    "  final_loss = 0.0\n",
    "  for epoch in range(2000):\n",
    "    net.zero_grad()\n",
    "\n",
    "    ### TODO ###\n",
    "    # Update the target network with the previous weights from net\n",
    "\n",
    "    ############\n",
    "\n",
    "    with torch.no_grad():\n",
    "      Jnext = target_net.forward(Xnext)\n",
    "      Jd, ind = torch.min(G + 0.95*Jnext, dim=1) # we discount the future cost-to-go\n",
    "    J = net.forward(X)\n",
    "    loss = loss_function(J, Jd)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 199 == 0:\n",
    "      print('[%d] loss: %.6f' % (epoch + 1, loss.item()))\n",
    "      plot_and_compare(net, timestep, min_time=min_time)\n",
    "\n",
    "  final_loss = loss.item()\n",
    "  return net, final_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FE1bDAwmFJdD"
   },
   "source": [
    "## Training the Networks\n",
    "\n",
    "Now that we defined the training loop, we can train our models! Feel free to play with the hyperparameters and the seed. \n",
    "\n",
    "**For the autograder to work, change the two cells below back to their original state.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tK8lpObpVy3t"
   },
   "outputs": [],
   "source": [
    "# For the autograder to work, you do not need to modify this cell!\n",
    "set_seed(12345)\n",
    "net_quadratic = Net()\n",
    "target_net_quadratic = Net()\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = optim.SGD(net_quadratic.parameters(), lr=0.01)\n",
    "net_quadratic, final_loss_quadratic = solve(net_quadratic, target_net_quadratic, loss_function, optimizer, discount=0.95, min_time=False)\n",
    "print(\"Final loss: \", final_loss_quadratic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "icJyOekUkU0R"
   },
   "outputs": [],
   "source": [
    "# For the autograder to work, you do not need to modify this cell!\n",
    "set_seed(12345)\n",
    "net_min_time = Net()\n",
    "target_net_min_time = Net()\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = optim.SGD(net_min_time.parameters(), lr=0.0015, momentum=0.9)\n",
    "net_min_time, final_loss_min_time = solve(net_min_time, target_net_min_time, loss_function, optimizer, discount=0.95, min_time=True)\n",
    "print(\"Final loss: \", final_loss_min_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n7xbR3EEOXlI"
   },
   "source": [
    "## Extracting the Policy\n",
    "\n",
    "Remember that the cost-to-go implicitly encodes our policy - if we can score every state, we should know exactly where we want to go! Now that we trained a neural network to approximate the cost-to-go $J(x)$, we should be able to use this approximation to extract a policy and control our system. In the cell below, your job is to fill in the code where we have marked `### TODO ###`. Generate the cost-to-go for all future states reachable from your current state using your actions, and then use those to pick the best action to take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uERy256Z9Ajv"
   },
   "outputs": [],
   "source": [
    "def extract_policy(minimum_time=False):\n",
    "\n",
    "    if minimum_time:\n",
    "      net = net_min_time\n",
    "    else:\n",
    "      net = net_quadratic\n",
    "    net.eval()\n",
    "    \n",
    "    state_grid = [set(x1s.data.numpy()), set(x2s.data.numpy())]\n",
    "    state_mesh = BarycentricMesh(state_grid)\n",
    "\n",
    "    # Re-define the double integrator\n",
    "    A = np.array([[0., 1.], [0., 0.]])\n",
    "    B = np.array([[0., 1.]])\n",
    "\n",
    "    # Loop through each state, and find the best action\n",
    "    best_actions = np.empty((1, num_states)) # array containing the best actions\n",
    "    for s in range(num_states):\n",
    "      state = torch.tensor(state_mesh.get_mesh_point(s)).float()\n",
    "      next_js = np.empty((1, num_actions))\n",
    "      for u in range(num_actions):\n",
    "        action = us[u].item()\n",
    "\n",
    "        ### TODO ###\n",
    "        # Generate the cost-to-go for the next states\n",
    "        next_js[0][u] = 0 # placeholder\n",
    "        ############\n",
    "\n",
    "      ### TODO ###\n",
    "      # Pick the best action\n",
    "      best_actions[0][s] = 0 # placeholder \n",
    "      ############\n",
    "\n",
    "    policy = BarycentricMeshSystem(state_mesh, best_actions)\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gd1TfDuyPJnq"
   },
   "source": [
    "## Simulating the System\n",
    "\n",
    "In the cell below we've plugged our policy into a drake diagram. We simulate the minimum time cost policy, and the quadratic regulator cost policy. There is nothing to implement here, but this is another opportunity to familiarize yourselves with drake so definitely take a look!\n",
    "\n",
    "The basic workflow involves instantiating a drake `diagram`, and adding `systems` to it. Systems can be pre-defined in drake (like the visualizer we use below for the double integrator), or we can define them ourselves (like the double integrator plant we define as a custom `LinearSystem`, or the policy which we implement as a `BarycentricMeshSystem` above). Once systems are added, we can connect their inputs and outputs to other systems to dictate the flow of information within the diagram. Lastly, we pass our diagram to `simulator` which will simulate and visualize the evolution of our overall system in time. \n",
    "\n",
    "Feel free to play with the starting state, and the simulation duration.\n",
    "\n",
    "**For the autograder to work, change back to the original starting state ([3., 3.]) and the original simulation duration (20)!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3y6FYW3EfptL"
   },
   "outputs": [],
   "source": [
    "def simulate_policy(minimum_time=False):\n",
    "  # Feel free to try different starting states\n",
    "  start_state = [3., 3.] # set to [3, 3] for the autograder!\n",
    "\n",
    "  # Extract the policy, and plot it\n",
    "  policy = extract_policy(minimum_time=minimum_time)\n",
    "  fig = plt.figure(1, figsize=(9, 4))\n",
    "  ax = plt.axes(projection='3d')\n",
    "  ax.set_xlabel(\"q\")\n",
    "  ax.set_ylabel(\"qdot\")\n",
    "  if minimum_time:\n",
    "    ax.set_title(\"Minimum Time Policy\")\n",
    "  else:\n",
    "    ax.set_title(\"Quadratic Policy\")\n",
    "  [Q, Qdot] = np.meshgrid(x1s.data.numpy(), x2s.data.numpy())\n",
    "  Pi = np.reshape(policy.get_output_values(), Q.shape)\n",
    "  surf = ax.plot_surface(Q, Qdot, Pi, rstride=1, cstride=1, cmap=cm.jet)\n",
    "\n",
    "  # Wire up the drake diagram\n",
    "  double_integrator_plant = LinearSystem(A=np.mat('0 1; 0 0'),\n",
    "                                         B=np.mat('0; 1'),\n",
    "                                         C=np.eye(2),\n",
    "                                         D=np.zeros((2,1)))\n",
    "  builder = DiagramBuilder() # instantiate a diagram builder\n",
    "  plant = builder.AddSystem(double_integrator_plant) # add a sub-system to it\n",
    "  vi_policy = builder.AddSystem(policy)\n",
    "  builder.Connect(plant.get_output_port(0), vi_policy.get_input_port(0)) # connecting inputs/outputs of two sub-systems in our diagram\n",
    "  builder.Connect(vi_policy.get_output_port(0), plant.get_input_port(0))\n",
    "  visualizer = builder.AddSystem(DoubleIntegratorVisualizer(show=plt_is_interactive))\n",
    "  builder.Connect(plant.get_output_port(0), visualizer.get_input_port(0))\n",
    "  diagram = builder.Build() # finish building the diagram\n",
    "\n",
    "  # Simulate the system\n",
    "  simulator = Simulator(diagram)\n",
    "  simulator.get_mutable_context().SetContinuousState(start_state) # set the initial state\n",
    "  AdvanceToAndVisualize(simulator, visualizer, 20.) \n",
    "\n",
    "  return simulator.get_mutable_context().get_continuous_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1G4acSVHj2g4"
   },
   "outputs": [],
   "source": [
    "# Simulate the minimum time policy\n",
    "final_state_min_time = simulate_policy(minimum_time=True)\n",
    "print(\"Final State: \", final_state_min_time[0], final_state_min_time[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sH_wZGUhnpxr"
   },
   "outputs": [],
   "source": [
    "# Simulate the quadratic cost policy\n",
    "final_state_quadratic = simulate_policy(minimum_time=False)\n",
    "print(\"Final State: \", final_state_quadratic[0], final_state_quadratic[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NGpqwd5E7bcy"
   },
   "source": [
    "## Written Questions\n",
    "\n",
    "1) Although it gets close, why does the system not stabilize exactly at the origin?\n",
    "\n",
    "2) In this notebook we implemented \"fitted value iteration\". In terms of the problem setting, why would we use \"fitted value iteration\" instead of graph search? By problem setting we mean: discrete/continuous states, discrete/continuous actions, discrete/continuous time.\n",
    "\n",
    "3) For our implementation, we assume deterministic transitions between states. This means that given a particular state $x_t$ and action $u_t$, we will always end up in the same unique $x_{t+1}$. Mathematically:\n",
    "\n",
    "$$x_{t+1} = f(x_t, a_t)$$\n",
    "\n",
    "In real world scenarios we often have to deal with things like noisy actuation, which can cause our state transitions to be stochastic. Mathematically:\n",
    "\n",
    "$$P(x_{t+1} | x_t, a_t) = f(x_t, a_t)$$\n",
    "\n",
    "If we had stochastic transitions, which step of the algorithm would need to change, and how would it change?\n",
    "\n",
    "4) In our implementation, we discount the future cost-to-go by a constant $\\gamma = 0.95$ (take a look at the `solve` method). Consider the policy evaluation equation used to evaluate a policy $u = \\pi(x)$:\n",
    "\n",
    "$$J(x) = g(x, \\pi(x)) + \\gamma J(f(x, \\pi(x)))$$\n",
    "\n",
    "For $\\gamma = 1.0$ notice that if cost-to-go $J(x)$ is a solution to the policy evaluation equation, then $J(x) + C$ for any constant $C$ will also be a solution to this equation. If $0 < \\gamma < 1$, is this still the case? Based on your answer, would you expect discounting to help or hurt our accuracy in fitting the optimal cost-to-go function $J^*(x)$?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VejEvzHyHdQ-"
   },
   "source": [
    "## Autograding\n",
    "The autograder has two tests:\n",
    "\n",
    "1) Test that the final loss generated for both the quadratic cost and the minimum time cost is within some range.\n",
    "\n",
    "2) Test that the generated policies land the system within the right final state range after simulation. \n",
    "\n",
    "You can check your work by running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DexQbX3eI55O"
   },
   "outputs": [],
   "source": [
    "from underactuated.exercises.dp.fitted_double_integrator.test_fitted_double_integrator import TestFittedDoubleIntegrator\n",
    "from underactuated.exercises.grader import Grader\n",
    "Grader.grade_output([TestFittedDoubleIntegrator], [locals()], 'results.json')\n",
    "Grader.print_test_results('results.json')"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "colab": {
   "collapsed_sections": [],
   "name": "fitted_double_integrator.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
