{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# ROA Estimation for the Time-Reversed Van der Pol Oscillator",
   "metadata": {
    "id": "fDy8gduFOREC",
    "cell_id": "123ac2b9-aaeb-4abb-9cbf-fcb3c6631999",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hS1y-6AWOREH",
    "cell_id": "00003-29acff14-0610-4b5c-b102-da623ae6a20c",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "48b39413",
    "execution_start": 1646256465549,
    "execution_millis": 1004,
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "source": "# python libraries\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\n# pydrake imports\nfrom pydrake.all import (LinearQuadraticRegulator, MathematicalProgram, Variables,\n                         Solve, RealContinuousLyapunovEquation)\nfrom pydrake.examples.van_der_pol import VanDerPolOscillator\n\n# underactuated imports\nfrom underactuated import plot_2d_phase_portrait\n\n# increase default size matplotlib figures\nfrom matplotlib import rcParams\nrcParams['figure.figsize'] = (6, 6)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Problem Description\nIn this notebook we will study the time-reversed Van der Pol oscillator.\nThe equations of motion for this system are polynomial, and read as follows:\n\n$$\\begin{aligned}\\dot x_1 &= - x_2, \\\\ \\dot x_2 &= x_1 + (x_1^2 - 1) x_2.\\end{aligned}$$\n\nWe compactly represent the latter as $\\dot{\\mathbf{x}} = f(\\mathbf{x})$, with $\\mathbf{x} = [x_1, x_2]^T$.",
   "metadata": {
    "id": "jhisk7tXOREH",
    "cell_id": "00004-3fa35b45-b964-45e3-86d0-66156cba100c",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0DfL2CoHOREI",
    "cell_id": "00005-c477c065-78fd-4299-bce9-3d46a381b4e0",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "b3959b58",
    "execution_start": 1646256466557,
    "execution_millis": 3,
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "source": "# function that implements the time-reversed Van der Pol dynamics\nf = lambda x: [- x[1], x[0] + (x[0]**2 - 1) * x[1]]",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Here is the phase portrait of the time-reversed Van der Pol oscillator.\n\nAs you can see, the origin of this system is locally asymptotically stable whereas, outside the Region Of Attraction (ROA) of the origin, the trajectories escape to infinity.\nThe boundary of the ROA is an *unstable periodic orbit*:\nif the system state at time $t=0$ is exactly on this curve, the oscillator will orbit around the origin forever.\nHowever, any disturbance will make the system either converge to the origin or escape to infinity.\nNotice that the shape of this ROA is nontrivial (it is not even a convex set) and no analytic description of it is available.\n\n**Note:**\nReversing the sign of $f$, we obtain the [classical Van der Pol oscillator](https://en.wikipedia.org/wiki/Van_der_Pol_oscillator); for which the above periodic orbit is a (globally) asymptotically stable [limit cycle](http://underactuated.mit.edu/simple_legs.html#section1) and the origin is an unstable equilibrium.\nHere we reverse time (i.e. change the sign of $f$) to make the origin a stable equilibrium.",
   "metadata": {
    "id": "oYonK-zMOREI",
    "cell_id": "00006-705edee4-a7a4-4d8f-92fa-66bd2fcca0a0",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "AzyXYSnkOREJ",
    "cell_id": "00007-c7c813c6-450f-4c77-a293-bdd0692455a4",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "4a379c87",
    "execution_start": 1646256466567,
    "execution_millis": 984,
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "source": "# compute and plot the unstable periodic orbit\nlimit_cycle = VanDerPolOscillator.CalcLimitCycle()\nplt.plot(limit_cycle[0], limit_cycle[1], color='b', linewidth=3, label='ROA boundary')\nplt.legend(loc=1)\n\n# plot the phase portrait\nxlim = (-3, 3)\nplot_2d_phase_portrait(f, x1lim=xlim, x2lim=xlim)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "In this notebook we will use Sums-Of-Squares (SOS) optimization to find an inner approximation of the ROA of the equilibrium point in the origin.\nWe will write three different SOS optimizations, and we will analyze their pros and cons.",
   "metadata": {
    "id": "3b-1NEzIOREJ",
    "cell_id": "00008-f9cf2c93-167a-4019-b3fe-7486e8a8dc88",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## Lyapunov Function via Linearization\nThe approach we will follow to estimate the ROA of the oscillator is the following:\n\n- We linearize the dynamics $\\dot{\\mathbf{x}} = f(\\mathbf{x})$ around the origin, to get $\\dot{\\mathbf{x}} = A \\mathbf{x}$.\n\n- We solve the Lyapunov equation $$A^T P + P A = - Q$$ to get the matrix $P$, and the Lyapunov function $V(\\mathbf{x}) = \\mathbf{x}^T P \\mathbf{x}$ for the linearized system.\n\n- Lyapunov theory tells us that if $A$ is strictly stable (all its eigenvalues have strictly negative real part) then the origin is a locally asymptotically stable equilibrium point for the nonlinear system $\\dot{\\mathbf{x}} = f(\\mathbf{x})$.\nMoreover, a conservative approximation of the ROA can be obtained using the Lyapunov function $V(\\mathbf{x})$ derived in the linear analysis.\n\n- We consider the level sets $$L(\\rho) = \\{ \\mathbf{x} : V(\\mathbf{x}) \\leq \\rho \\},$$ and we look for the maximum value of $\\rho$ such that $$\\dot{V}(\\mathbf{x}) = \\frac{\\partial V}{\\partial \\mathbf{x}} f(\\mathbf{x}) = 2 \\mathbf{x}^T P f(\\mathbf{x}) < 0, \\quad \\forall \\mathbf{x} \\in L(\\rho)\\backslash \\{0\\}.$$\nIn words, we try to find the largest level set $L(\\rho)$ entirely contained the region of space where $\\dot{V}(\\mathbf{x})$ is negative.\nLyapunov theory tells us that any trajectory that starts inside such a set will eventually converge to the origin.\n\nWe start by deriving $V(\\mathbf{x})$.",
   "metadata": {
    "id": "MsNSVypnOREK",
    "cell_id": "00009-57f6e5ae-5f3e-4a87-b8ba-53501c8c333c",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hs3AAy5xOREK",
    "cell_id": "00010-6faf3608-9117-42e8-9023-1637374a6ce8",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "5b7f1387",
    "execution_start": 1646256467549,
    "execution_millis": 3,
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "source": "# linear approximation A x of f(x)\n# for x sufficiently close to the origin\n# (if you don't see this immediately, do the math!)\nA = np.array([[0, -1], [1, -1]])\n\n# rhs of the Lyapunov equation (standard choice)\nQ = np.eye(2)\n\n# positive definite matrix of the Lyapunov function\nP = RealContinuousLyapunovEquation(A, Q)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Manual Check\nThe advantage of working in 2D is that we can plot things!\nBefore starting with complicated optimizations, let us plot $V(\\mathbf{x})$ and $\\dot{V}(\\mathbf{x})$ to get a sense of what we are actually looking for in this analysis.",
   "metadata": {
    "id": "nXM2YRffOREL",
    "cell_id": "00011-455e27a6-0290-4479-b74f-b59a5449d104",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1IOxgscrOREL",
    "cell_id": "00012-81725722-75bd-4ddc-931c-eb2524194717",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "cd1f01fc",
    "execution_start": 1646256467555,
    "execution_millis": 2,
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "source": "# function that given rho plots the boundary\n# of the the set L(rho) defined above\ndef plot_V(rho):\n    \n    # grid of the state space\n    x1 = np.linspace(*xlim)\n    x2 = np.linspace(*xlim)\n    X1, X2 = np.meshgrid(x1, x2)\n    \n    # function that evaluates V(x) at a given x\n    # (looks bad, but it must accept meshgrids)\n    eval_V = lambda x: sum(sum(x[i]*x[j]*Pij for j, Pij in enumerate(Pi)) for i, Pi in enumerate(P))\n    \n    # contour plot with only the rho level set\n    cs = plt.contour(X1, X2, eval_V([X1, X2]), levels=[rho], colors='r', linewidths=3, zorder=3)\n    \n    # misc plot settings\n    plt.xlabel(r'$x_1$')\n    plt.ylabel(r'$x_2$')\n    plt.gca().set_aspect('equal')\n    \n    # fake plot for legend\n    plt.plot(0, 0, color='r', linewidth=3, label=r'$\\{ \\mathbf{x} : V(\\mathbf{x}) = \\rho \\}$')\n    plt.legend()\n    \n    return cs\n    \n# function that plots the levels sets of Vdot(x)\ndef plot_Vdot():\n    \n    # grid of the state space\n    x1 = np.linspace(*xlim)\n    x2 = np.linspace(*xlim)\n    X1, X2 = np.meshgrid(x1, x2)\n    \n    # function that evaluates Vdot(x) at a given x\n    eval_Vdot = lambda x: 2*sum(sum(x[i]*f(x)[j]*Pij for j, Pij in enumerate(Pi)) for i, Pi in enumerate(P))\n    \n    # contour plot with only the rho level set\n    cs = plt.contour(X1, X2, eval_Vdot([X1, X2]), colors='b', levels=np.linspace(-10, 40, 11))\n    plt.gca().clabel(cs, inline=1, fontsize=10)\n    \n    # misc plot settings\n    plt.xlabel(r'$x_1$')\n    plt.ylabel(r'$x_2$')\n    plt.gca().set_aspect('equal')\n    \n    # fake plot for legend\n    plt.plot(0, 0, color='b', label=r'$\\dot{V}(\\mathbf{x})$')\n    plt.legend()\n    \n    return cs",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "By playing with the code below, we see that the largest $\\rho$ we can find is $\\approx 2.3$.",
   "metadata": {
    "id": "ibIgYQPBOREL",
    "cell_id": "00013-047f55a6-17db-43d7-b07e-ab770097de73",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "V2SAhIiJOREL",
    "cell_id": "00014-fb05ac2f-7adb-43ee-9c1d-fdbd0cf2a83a",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "8704eb86",
    "execution_start": 1646256467559,
    "execution_millis": 488,
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "source": "# tune rho by hand to make it as big as possible\n# while staying in the region where Vdot(x) is negative\nrho_max = 2.3\n\n# plot Vdot(x) and V(x) = rho\nVdot_cs = plot_Vdot()\nV_cs = plot_V(rho_max)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Superimposing the level set to the phase portrait, we see that this is a pretty good approximation of the ROA.",
   "metadata": {
    "id": "IZAaj9GpOREM",
    "cell_id": "00015-a64e6ba9-4a47-447b-87a3-f885034bbd6d",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GVm8s34NOREM",
    "cell_id": "00016-04bbc97e-4aec-4693-b81b-6b27d6d611cd",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "8db6c40c",
    "execution_start": 1646256468040,
    "execution_millis": 956,
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "source": "plot_2d_phase_portrait(f, x1lim=xlim, x2lim=xlim)\nplot_V(rho_max)\nplt.plot(limit_cycle[0], limit_cycle[1], color='b', linewidth=3, label='ROA boundary')\nplt.legend(loc=1)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Of course, when $\\text{dim}(\\mathbf{x}) > 2$, a \"manual\" maximization of $\\rho$ has no hope to work.\nThat's where SOS programming really makes the difference!\nThe goal of this notebook is to experiment these tools on a case where things can actually be visualized, so that we get a better sense of the power of this technique.",
   "metadata": {
    "id": "3LTO_6vwOREM",
    "cell_id": "00017-f90775f9-2978-4926-9c86-e46416010500",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## Method 1: Line-Search on $\\rho$\nThe first method we use to estimate the ROA is the one from the textbook example \"[Region of attraction for the one-dimensional cubic system](http://underactuated.mit.edu/lyapunov.html#roa_cubic_system)\".\nWe look for the largest $\\rho$ for which there exists a SOS polynomial $\\lambda(\\mathbf{x})$ such that $$- \\dot{V}(\\mathbf{x}) - \\lambda(\\mathbf{x}) (\\rho - V(\\mathbf{x})) - \\epsilon \\mathbf{x}^T \\mathbf{x} \\ \\text{is SOS},$$\nwith $\\epsilon$ very small.\nThis problem cannot be written as a single SOS program, since both the polynomial $\\lambda$ and scalar $\\rho$ are decision variables, and here they multiply.\nHence we naively solve a sequence of SOS programs with increasing value of $\\rho$.\n\nThe intuition behind this formulation is the following.\nThink of the condition \"is SOS\" as \"$\\geq 0$\" (actually, the first is sufficient for the second).\nThen what we are asking is $- \\dot{V}(\\mathbf{x}) \\geq \\lambda(\\mathbf{x}) (\\rho - V(\\mathbf{x})) + \\epsilon \\mathbf{x}^T \\mathbf{x}$.\nInside the level set $L(\\rho)$, we have $\\rho - V(\\mathbf{x}) \\geq 0$ and, since $\\lambda(\\mathbf{x})$ is SOS, $\\lambda(\\mathbf{x}) (\\rho - V(\\mathbf{x})) \\geq 0$.\nThus the condition above is just sayng that, for all $\\mathbf{x}$ in $L(\\rho)$, we must have $- \\dot{V}(\\mathbf{x})\\geq \\epsilon \\mathbf{x}^T \\mathbf{x}$, i.e., $\\dot{V}(\\mathbf{x})$ negative definite.",
   "metadata": {
    "id": "2SZpydhEOREM",
    "cell_id": "00018-893b8158-44d9-4190-9896-8fcfc2444d5f",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "d0QBuEAXOREM",
    "cell_id": "00019-8cf1cfe0-afb2-466e-aa6f-e8c68d3132e7",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "81bfd7c6",
    "execution_start": 1646256468987,
    "execution_millis": 13,
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "source": "# function that verifies the condition described above\n# for the level set L(rho) for a given rho\ndef is_verified(rho):\n    \n    # initialize optimization problem\n    # (with Drake there is no need to specify that\n    # this is going to be a SOS program!)\n    prog = MathematicalProgram()\n    \n    # SOS indeterminates\n    x = prog.NewIndeterminates(2, 'x')\n    \n    # Lyapunov function\n    V = x.dot(P).dot(x)\n    V_dot = 2*x.dot(P).dot(f(x))\n    \n    # degree of the polynomial lambda(x)\n    # no need to change it, but if you really want to,\n    # keep l_deg even (why?) and do not set l_deg greater than 10\n    # (otherwise optimizations will take forever)\n    l_deg = 4\n    assert l_deg % 2 == 0\n\n    # SOS Lagrange multipliers\n    l = prog.NewSosPolynomial(Variables(x), l_deg)[0].ToExpression()\n    \n    # main condition above\n    eps = 1e-3 # do not change\n    prog.AddSosConstraint(- V_dot - l * (rho - V) - eps*x.dot(x))\n    \n    # solve SOS program\n    # no objective function in this formulation\n    result = Solve(prog)\n    \n    # return True if feasible, False if infeasible\n    return result.is_success()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Now that we have the building block of our algorithm, it's your time to write the line search to find the maximum $\\rho$ for which the condition described above holds.\n\nImplement the line search in next cell.\nStart with `rho = 0`, check if the level set $L(\\rho)$ is verified (i.e. the function `is_verified(rho)` returns `True`); if yes, increase `rho` by `rho_step = .01`, if no, assign to the variable `rho_method_1` the maximum verified `rho` you've found with this procedure.",
   "metadata": {
    "id": "yPTCO3Z4OREN",
    "cell_id": "00020-7ffa0ae5-cf9c-4049-8a23-4c7653da283b",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "p_9yOUTlOREN",
    "cell_id": "00021-e699449d-434e-4d45-82f7-ae204dd1b525",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "e724d709",
    "execution_start": 1646256469001,
    "execution_millis": 7,
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "source": "# line-search parameters\nrho = 0 # do not modify\nrho_step = .01 # do not modify\n\n# implement your line-search here\n# modify here\n\n# set the maximum value of rho you've found with line search\nrho_method_1 = 0 # modify here\n    \n# print maximum rho\nprint(f'Method 1 verified rho = {rho_method_1}.')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Did this method do a good job in approximating (from below) the maximum $\\rho$ we have found by hand?",
   "metadata": {
    "id": "ghOGCjqiOREN",
    "cell_id": "00022-3d33eb76-b629-44db-b102-6db90b0a1dd6",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## Method 2: Single-Shot SOS Program\nWith the previous formulation we had to solve a sequence of SOS programs, now we consider an equivalent formulation of the SOS problem in which we can directly maximize $\\rho$.\nIn the previous case we wrote a SOS program to check the impication\n$$\\mathbf{x} \\in L(\\rho) \\Rightarrow \\dot{V}(\\mathbf{x}) < 0.$$\nThis, however, can be equivalently stated as\n$$\\dot{V}(\\mathbf{x}) \\geq 0 \\Rightarrow \\mathbf{x} \\not\\in L(\\rho).$$\nExpressing $\\mathbf{x} \\not\\in L(\\rho)$ as $V(\\mathbf{x}) - \\rho \\geq 0$, it turns out that the latter condition can be verified with a single SOS program.\n(Actually, we should say $V(\\mathbf{x}) - \\rho > 0$, but working with a computer there is no difference.)\n\nThe new problem reads as follows.\nFind an SOS polynomial $\\lambda(\\mathbf{x})$ such that\n$$V(\\mathbf{x}) - \\rho - \\lambda(\\mathbf{x}) \\dot{V}(\\mathbf{x}) \\ \\text{is SOS}.$$\nIn fact, this implies $V(\\mathbf{x}) - \\rho \\geq \\lambda(\\mathbf{x}) \\dot{V}(\\mathbf{x})$ and, for all $\\mathbf{x}$ where $\\dot{V}(\\mathbf{x}) \\geq 0$, we get $V(\\mathbf{x}) - \\rho \\geq 0$.\n\nNotice that now $\\lambda$ does not multiply $\\rho$, and we can search over both of them at the same time.\nHence we can ask the optimizer to maximize $\\rho$.\nThere is however an issue with the current problem formulation...\n\n### Not quite there yet...\n\nDo you see anything wrong with the problem formulation we put together so far? What do you think the maximum $\\rho$ will be?\n\nAs stated so far, the problem will always return $\\rho = 0$!\nTo see why, first notice that for $\\rho = \\lambda = 0$ the SOS condition above would become $V(\\mathbf{x})$ is SOS, which holds since $V(\\mathbf{x}) = \\mathbf{x}^T P \\mathbf{x}$.\nNow consider a positive $\\rho$.\nSince $V(0) = \\dot{V} (0) = 0$, evaluating the SOS condition in the origin, we would get $-\\rho \\geq 0$ which can never hold!\n\nNot everything is lost, we have a neat fix for you.\nCertainly, if $V(\\mathbf{x}) - \\rho$ is nonnegative, so is $\\mathbf{x}^T\\mathbf{x}(V(\\mathbf{x}) - \\rho)$.\nHence we consider the SOS condition\n$$\\mathbf{x}^T\\mathbf{x}(V(\\mathbf{x}) - \\rho) - \\lambda(\\mathbf{x}) \\dot{V}(\\mathbf{x}) \\ \\text{is SOS},$$\nwith $\\lambda(\\mathbf{x})$ SOS.\nNow the issue in the origin is fixed, since for $\\mathbf{x} = 0$, we get \"$0$ is SOS\", which is always true.\nMoreover, where $\\mathbf{x}$ is such that $\\dot{V}(\\mathbf{x}) \\geq 0$, the new SOS condition requires $\\mathbf{x}^T\\mathbf{x}(V(\\mathbf{x}) - \\rho) \\geq 0$ and hence $V(\\mathbf{x}) - \\rho \\geq 0$ as desired.\n\nNow we are good to go!",
   "metadata": {
    "id": "uW4ztJFrOREN",
    "cell_id": "00023-6ccac0cc-0699-4a36-8296-b12334557dd3",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "In the next cell you need to code the SOS program we just described.\nWe have already set up the problem for you.\nYou only have to write two lines of code:\n\n- A line where you add the SOS constraint described in the \"Not quite there yet...\" subsection above.\nTo do this, use the method `AddSosConstraint` of `MathematicalProgram` (same method we've used in the previous case).\n\n- A line where you set the objective function of the SOS program.\nRemember that we'd like to maximize `rho`.\nTo this end, use the method `AddLinearCost` of `MathematicalProgram`, but notice that writing `prog.AddLinearCost(rho)` the variable `rho` will be *minimized*.\nAny idea for a quick workaround?\nHint: it shouldn't take more than one character!",
   "metadata": {
    "id": "qTD5kRdwOREO",
    "cell_id": "00024-71ad3f62-a2a8-45c7-9cf5-66cf81b226c7",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TiW1IZY6OREO",
    "cell_id": "00025-b0ec0f00-b640-4ddc-86b7-c886fa367930",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "9802e689",
    "execution_start": 1646256469002,
    "execution_millis": 0,
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "source": "# initialize optimization problem\nprog2 = MathematicalProgram()\n\n# SOS indeterminates\nx = prog2.NewIndeterminates(2, 'x')\n\n# Lyapunov function\nV = x.dot(P).dot(x)\nV_dot = 2*x.dot(P).dot(f(x))\n\n# degree of the polynomial lambda(x)\n# no need to change it, but if you really want to,\n# keep l_deg even and do not set l_deg greater than 10\nl_deg = 4\nassert l_deg % 2 == 0\n\n# SOS Lagrange multipliers\nl = prog2.NewSosPolynomial(Variables(x), l_deg)[0].ToExpression()\n\n# level set as optimization variable\nrho = prog2.NewContinuousVariables(1, 'rho')[0]\n\n# write here the SOS condition described in the \"Not quite there yet...\" section above\n# modify here\n\n# insert here the objective function (maximize rho)\n# modify here\n\n# solve program only if the lines above are filled\nif len(prog2.GetAllConstraints()) != 0:\n\n    # solve SOS program\n    result = Solve(prog2)\n\n    # get maximum rho\n    assert result.is_success()\n    rho_method_2 = result.GetSolution(rho)\n\n    # print maximum rho\n    print(f'Method 2 verified rho = {rho_method_2}.')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Method 3: Smarter Single-Shot SOS Program\nThe SOS program we just wrote was already a satisfying solution, but it turns out we can do even better!\nFrom the textbook chapter \"[Lyapunov analysis with convex optimization](http://underactuated.mit.edu/lyapunov.html#section2)\", you know that every SOS constraint brings with it a lot of optimization variables and an SDP constraint.\nSo, whenever we can, removing redundant SOS requirements is always a good thing to do.\n\nWe claim that in the previous formulation we don't need $\\lambda(\\mathbf{x})$ to be SOS. How is this possible?\n\nWe start by noticing that, in a neighborhood of the origin, excluded the origin itself, $\\dot{V}(\\mathbf{x})$ is strictly negative.\n(This because $V(\\mathbf{x})$ is a Lyapunov function for the linearized system hence, locally, it works also for the nonlinear system.)\n\nIn light of the latter observation, instead of asking that $\\dot{V}(\\mathbf{x})$ is negative for all $\\mathbf{x} \\neq 0$ in $L(\\rho)$, we can equivalently ask that all the points $\\mathbf{x} \\neq 0$ where $\\dot{V}(\\mathbf{x}) = 0$ must be outside the level set $L(\\rho)$.\nThis might take a second to parse!\n\nThe latter condition can be enforced exactly as the one above:\n$$\\mathbf{x}^T\\mathbf{x}(V(\\mathbf{x}) - \\rho) - \\lambda(\\mathbf{x}) \\dot{V}(\\mathbf{x}) \\ \\text{is SOS},$$\nbut this time we do not require $\\lambda(\\mathbf{x})$ to be SOS.\n\nHere is the reasoning.\nFirst, notice that this condition implies $\\mathbf{x}^T\\mathbf{x}(V(\\mathbf{x}) - \\rho) \\geq \\lambda(\\mathbf{x}) \\dot{V}(\\mathbf{x})$.\nThen, observe that for all $\\mathbf{x}$ such that $\\dot{V}(\\mathbf{x}) = 0$, we get $\\mathbf{x}^T\\mathbf{x}(V(\\mathbf{x}) - \\rho) \\geq 0$.\nThis implies $V(\\mathbf{x}) - \\rho \\geq 0$, i.e., $\\mathbf{x} \\not\\in L(\\rho)$ as desired.\n(As before, no need to care about what happens at the boundary of the level set.)\n\nThis trick can make a huge difference when you need to verify high-dimensional systems!",
   "metadata": {
    "id": "pR1-_e_BOREO",
    "cell_id": "00026-1b3ba859-6a3e-4d14-9544-2b325d09be7f",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "To try this new idea:\n- In the following cell, copy and paste the optimization problem you just wrote above for \"Method 2\". Attention: this time give the name `prog3` to the `MathematicalProgram` you write (important for autograding).\n\n- Substitute the definition of the polynomial $\\lambda$ from `l = prog.NewSosPolynomial(Variables(x), l_deg)[0].ToExpression()` to `l = prog.NewFreePolynomial(Variables(x), l_deg).ToExpression()`.\n\n- Run the new SOS program.\n\n- Define the variable `rho_method_3` to be the optimal value of `rho` for this new optimization problem.\n\nIf you have done thing correctly, `rho_method_3` should \"closely match\" `rho_method_2`!",
   "metadata": {
    "id": "1U613bxUOREO",
    "cell_id": "00027-da645d57-c16e-4495-8bfd-5b33e53b214d",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ze1Rhf6DOREO",
    "cell_id": "00028-3aeab4af-402d-4534-a988-d6aaa34318dc",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "f3456091",
    "execution_start": 1646256469016,
    "execution_millis": 10,
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "source": "# initialize optimization problem\nprog3 = MathematicalProgram()\n\nrho_method_3 = 0 # modify here\n\n# print maximum rho\nprint(f'Method 3 verified rho = {rho_method_3}.')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Final Note\nMore advanced techniques to approximate ROAs are available.\nGenerally they require some sort of alternation between optimizing over the multiplier $\\lambda$ (as we did here) and modifying the shape of the Lyapunov function $V(\\mathbf{x})$, e.g., by considering higher-order polynomials (here we stuck to the quadratic one coming from\nlinear analysis).\nThe level set $\\rho$ is generally kept fixed (e.g. equal to unity) since, when reshaping the Lyapunov function, the optimizer is allowed to scale the range of this function arbitrarily.\n\nHere is an image of SOS in its full glory approximating the ROA of the Van der Pol oscillator.\nImpressive, isn't it?!\n\n![figure](https://raw.githubusercontent.com/RussTedrake/underactuated/master/figures/exercises/van_der_pol_roa.png)\n(Courtesy of Shen Shen.)",
   "metadata": {
    "id": "K9IJWgJeOREO",
    "cell_id": "00029-6619fd64-3be4-479b-9381-c2545db80a37",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## Autograding\n\nYou can check your work by running the following cell:",
   "metadata": {
    "id": "g9lVqra7OREP",
    "cell_id": "00030-914d3023-c4fd-40ba-969a-5b1b592d1576",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Di4nJGYmOREP",
    "cell_id": "00031-ab12d2ec-ab7c-4174-a73d-7d4d4444db25",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "4611d00f",
    "execution_start": 1646256469017,
    "execution_millis": 595,
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "source": "from underactuated.exercises.lyapunov.van_der_pol.test_van_der_pol import TestVanDerPol\nfrom underactuated.exercises.grader import Grader\nGrader.grade_output([TestVanDerPol], [locals()], 'results.json')\nGrader.print_test_results('results.json')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=3f49d258-f819-4257-85d4-41b6587300a2' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "colab": {
   "name": "van_der_pol.ipynb",
   "provenance": []
  },
  "deepnote_notebook_id": "71cb95b0-7f3a-4690-a5db-3fe3530c700e",
  "deepnote": {},
  "deepnote_execution_queue": []
 }
}